#!/usr/bin/env python3
"""
Late Report Service
–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–ø–æ–∑–¥–∞–Ω–∏–π –∏–∑ –ø–æ—á—Ç—ã –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram
"""

import os
import re
import json
import imaplib
import email
import email.header
import io
import hashlib
import textwrap
import time
from pathlib import Path
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional, Tuple
import logging

import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from imapclient import IMAPClient
from dotenv import load_dotenv
import requests

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/late-report/late-report.log') if os.path.exists('/var/log/late-report') else logging.NullHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_config():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ env —Ñ–∞–π–ª–∞"""
    env_path = os.getenv('LATE_REPORT_ENV', '/etc/late-report/late-report.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
    else:
        load_dotenv('.env')
    
    attachment_regex = os.getenv('ATTACHMENT_NAME_REGEX', r'–°–æ–±–ª—é–¥–µ–Ω–∏–µ\s+—Å—Ä–æ–∫–æ–≤')
    # –ï—Å–ª–∏ regex –ø—É—Å—Ç–æ–π - –æ—Ç–∫–ª—é—á–∞–µ–º —Ñ–∏–ª—å—Ç—Ä
    if not attachment_regex or attachment_regex.strip() == '':
        attachment_regex = None
    
    return {
        'imap_host': os.getenv('YA_IMAP_HOST', 'imap.yandex.com'),
        'imap_user': os.getenv('YA_IMAP_USER'),
        'imap_pass': os.getenv('YA_IMAP_PASS'),
        'mailbox': os.getenv('YA_MAILBOX', 'INBOX'),
        'attachment_regex': attachment_regex,
        'tg_token': os.getenv('TG_TOKEN'),
        'tg_chat_id': os.getenv('TG_CHAT_ID'),
        'tg_topic_id_late': os.getenv('TG_TOPIC_ID_LATE', os.getenv('TG_TOPIC_ID', '26')),
        'tg_topic_id_docs': os.getenv('TG_TOPIC_ID_DOCS', '2'),
        'admin_chat_id': os.getenv('ADMIN_CHAT_ID'),
        'dry_run': os.getenv('DRY_RUN', '0').lower() in ('1', 'true', 'yes'),
        'send_if_empty': os.getenv('SEND_IF_EMPTY', 'false').lower() == 'true',
        'run_late_report': False if os.getenv('DOCS_ONLY', '0').lower() in ('1', 'true', 'yes') else os.getenv('RUN_LATE_REPORT', '1').lower() in ('1', 'true', 'yes'),
        'run_docs_report': True if os.getenv('DOCS_ONLY', '0').lower() in ('1', 'true', 'yes') else os.getenv('RUN_DOCS_REPORT', '1').lower() in ('1', 'true', 'yes'),
        'state_path': os.getenv('STATE_PATH', '/var/lib/late-report/state.json'),
        'state_file': os.getenv('STATE_FILE', '/opt/fuel-control/tools/late-report/state/processed.json'),
        'imap_lookback_days': int(os.getenv('IMAP_LOOKBACK_DAYS', '3')),
        'imap_max_uids': int(os.getenv('IMAP_MAX_UIDS', '500')),
        'report_tz': os.getenv('REPORT_TZ', 'Europe/Moscow'),
        'force_resend': os.getenv('FORCE_RESEND', '0').lower() in ('1', 'true', 'yes'),
        'dry_run': os.getenv('DRY_RUN', '0').lower() in ('1', 'true', 'yes'),
        'docs_only': os.getenv('DOCS_ONLY', '0').lower() in ('1', 'true', 'yes'),
        'docs_date_token': os.getenv('DOCS_DATE_TOKEN'),  # Override –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        'test_limit': int(os.getenv('TEST_LIMIT', '10')),
    }


def load_state(state_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–∏—Å–µ–º"""
    if os.path.exists(state_path):
        try:
            with open(state_path, 'r', encoding='utf-8') as f:
                state = json.load(f)
                # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                if 'processed_uids' not in state:
                    state['processed_uids'] = []
                if 'processed_file_hashes' not in state:
                    state['processed_file_hashes'] = []
                return state
        except Exception as e:
            logger.warning(f"Failed to load state: {e}")
    return {'processed_uids': [], 'processed_file_hashes': []}


def save_state(state_path: str, state: Dict):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
    try:
        state_dir = os.path.dirname(state_path)
        if state_dir:
            os.makedirs(state_dir, exist_ok=True)
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
    except PermissionError as e:
        logger.warning(f"Permission denied saving state to {state_path}: {e}. State will not be persisted.")
    except Exception as e:
        logger.error(f"Failed to save state: {e}")


def load_processed_keys(state_file: str) -> Dict[str, float]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π –∏–∑ state —Ñ–∞–π–ª–∞"""
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict):
                    # –§–æ—Ä–º–∞—Ç: {"key1": timestamp1, "key2": timestamp2, ...}
                    return data
                elif isinstance(data, list):
                    # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: ["key1", "key2", ...] - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict
                    return {key: 0.0 for key in data}
        except Exception as e:
            logger.warning(f"Failed to load processed keys from {state_file}: {e}")
    return {}


def save_processed_keys(state_file: str, processed_keys: Dict[str, float], max_age_days: int = 30, max_keys: int = 5000):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞"""
    try:
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        state_dir = os.path.dirname(state_file)
        if state_dir:
            os.makedirs(state_dir, exist_ok=True)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∫–ª—é—á–∏ (> max_age_days)
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 60 * 60
        filtered_keys = {
            key: timestamp
            for key, timestamp in processed_keys.items()
            if current_time - timestamp < max_age_seconds
        }
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_keys)
        if len(filtered_keys) > max_keys:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp –∏ –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_keys
            sorted_items = sorted(filtered_keys.items(), key=lambda x: x[1], reverse=True)
            filtered_keys = dict(sorted_items[:max_keys])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(filtered_keys, f, indent=2, ensure_ascii=False)
        
        logger.debug(f"Saved {len(filtered_keys)} processed keys to {state_file}")
    except PermissionError as e:
        logger.warning(f"Permission denied saving processed keys to {state_file}: {e}. Keys will not be persisted.")
    except Exception as e:
        logger.error(f"Failed to save processed keys: {e}")


def decode_filename(filename: Optional[str]) -> Optional[str]:
    """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ MIME –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    if not filename:
        return None
    
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è MIME –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å =?utf-8?B?...?=)
        decoded_parts = email.header.decode_header(filename)
        decoded_name = ''
        for part, encoding in decoded_parts:
            if isinstance(part, bytes):
                decoded_name += part.decode(encoding or 'utf-8', errors='replace')
            else:
                decoded_name += part
        return decoded_name
    except Exception as e:
        logger.warning(f"Failed to decode filename '{filename}': {e}, using as-is")
        return filename


def is_excel_file(filename: Optional[str], content_type: Optional[str] = None) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª Excel (–ø–æ –∏–º–µ–Ω–∏ –∏/–∏–ª–∏ content-type)"""
    if not filename:
        return False
    
    filename_lower = filename.lower()
    is_excel_by_name = filename_lower.endswith('.xlsx') or filename_lower.endswith('.xls')
    
    if is_excel_by_name:
        return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ content-type (–¥–∞–∂–µ –µ—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –Ω–µ .xlsx/.xls)
    if content_type:
        content_type_lower = content_type.lower()
        excel_types = [
            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'application/vnd.ms-excel',
            'application/octet-stream',
        ]
        if any(ct in content_type_lower for ct in excel_types):
            return True
    
    return False


def has_valid_delay_column(df: pd.DataFrame) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ '–û–ø–æ–∑–¥–∞–Ω–∏–µ, –º–∏–Ω.'"""
    cols = find_columns(df)
    return 'delay' in cols


def detect_report_type(df: pd.DataFrame) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á—ë—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ç–∞–±–ª–∏—Ü—ã"""
    cols_lower = {str(c).lower(): c for c in df.columns}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ late-report: –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ "–û–ø–æ–∑–¥–∞–Ω–∏–µ"
    for col_lower in cols_lower:
        if '–æ–ø–æ–∑–¥–∞–Ω' in col_lower:
            return 'late'
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ docs-report: –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ "–ü—Ä–∏—á–∏–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¢–¢–ù" –∏–ª–∏ "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É"
    for col_lower in cols_lower:
        if '–ø—Ä–∏—á–∏–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏' in col_lower or '—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' in col_lower:
            return 'docs'
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - unknown
    return 'unknown'


def find_docs_header_row(file_data: bytes) -> int:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞ '–û—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'
    
    –ò—â–µ—Ç —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –µ—Å—Ç—å "–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è" –∏ ("–ì–æ—Å. ‚Ññ –∞/–º" –∏–ª–∏ "–î–∞—Ç–∞ –¢–¢–ù"/"–ù–æ–º–µ—Ä –¢–¢–ù")
    """
    try:
        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 30 —Å—Ç—Ä–æ–∫ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        df_preview = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=None, nrows=30)
        
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –µ—Å—Ç—å "–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è" –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        for r in range(len(df_preview)):
            row_values = df_preview.iloc[r].astype(str).str.lower()
            row_str = ' '.join(row_values.values)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ "–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è"
            has_fio = '—Ñ–∏–æ' in row_str and ('–≤–æ–¥–∏—Ç–µ–ª' in row_str or '—Ñ–∏–æ' in row_str)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
            has_marker = (
                ('–≥–æ—Å' in row_str and ('‚Ññ' in row_str or '–Ω–æ–º–µ—Ä' in row_str) and ('–∞/–º' in row_str or '–∞–≤—Ç–æ' in row_str)) or
                ('–¥–∞—Ç–∞' in row_str and '—Ç—Ç–Ω' in row_str) or
                ('–Ω–æ–º–µ—Ä' in row_str and '—Ç—Ç–Ω' in row_str)
            )
            
            if has_fio and has_marker:
                logger.debug(f"Found docs header row {r}: –§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è + –º–∞—Ä–∫–µ—Ä")
                return r
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0 (fallback)
        logger.warning("Docs header row not found in first 30 rows (–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è + –º–∞—Ä–∫–µ—Ä), using header=0")
        return 0
    except Exception as e:
        logger.warning(f"Failed to find docs header row: {e}, using header=0")
        return 0


def normalize_text_value(value) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è: trim, –∑–∞–º–µ–Ω–∞ \xa0, —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤"""
    if pd.isna(value) or value is None:
        return ''
    text = str(value)
    # Trim
    text = text.strip()
    # –ó–∞–º–µ–Ω–∞ \xa0 –Ω–∞ –ø—Ä–æ–±–µ–ª
    text = text.replace('\xa0', ' ')
    # –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –¥–≤–æ–π–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = ' '.join(text.split())
    return text


def normalize_column_name(col_name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏: strip, lower, –∑–∞–º–µ–Ω–∞ –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤, —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ç–æ—á–∫–∏"""
    if pd.isna(col_name) or col_name is None:
        return ''
    text = str(col_name)
    # Trim
    text = text.strip()
    # Lower
    text = text.lower()
    # –ó–∞–º–µ–Ω–∞ \xa0 –Ω–∞ –ø—Ä–æ–±–µ–ª
    text = text.replace('\xa0', ' ')
    # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ
    text = text.rstrip('.')
    # –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –¥–≤–æ–π–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = ' '.join(text.split())
    return text


def find_fio_column(df: pd.DataFrame) -> Optional[str]:
    """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ –§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
    
    –ò—â–µ—Ç –∫–æ–ª–æ–Ω–∫—É, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ñ–∏–æ" –∏ ("–≤–æ–¥–∏—Ç–µ–ª" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "—Ñ–∏–æ")
    """
    for col in df.columns:
        col_normalized = normalize_column_name(str(col))
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ñ–∏–æ" –∏ ("–≤–æ–¥–∏—Ç–µ–ª" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "—Ñ–∏–æ")
        if '—Ñ–∏–æ' in col_normalized:
            if '–≤–æ–¥–∏—Ç–µ–ª' in col_normalized or col_normalized.strip() == '—Ñ–∏–æ':
                return col
    return None


def parse_docs_excel(file_data: bytes) -> pd.DataFrame:
    """–ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª–∞ –æ—Ç—á—ë—Ç–∞ '–û—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_row = find_docs_header_row(file_data)
        logger.debug(f"Reading docs Excel with header={header_row}")
        
        # –ß–∏—Ç–∞–µ–º Excel —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π –∑–∞–≥–æ–ª–æ–≤–∫–∞
        df = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=header_row)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        df = df.dropna(how='all')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
        df.columns = [normalize_column_name(str(col)) for col in df.columns]
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –≤ mapping)
        col_mapping = {normalize_column_name(str(col)): col for col in df.columns}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in df.columns:
            if df[col].dtype == 'object':  # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                df[col] = df[col].apply(normalize_text_value)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É"
        # –£–±–∏—Ä–∞–µ–º "0 —á–∞—Å–æ–≤, 0 –º–∏–Ω—É—Ç, 0 —Å–µ–∫—É–Ω–¥" –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É
        def clean_waiting_period(value):
            """–£–±–∏—Ä–∞–µ—Ç '0 —á–∞—Å–æ–≤, 0 –º–∏–Ω—É—Ç, 0 —Å–µ–∫—É–Ω–¥' –∏ –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É"""
            if pd.isna(value) or value is None:
                return ''
            text = str(value)
            
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã "0 —á–∞—Å–æ–≤/–º–∏–Ω—É—Ç/—Å–µ–∫—É–Ω–¥"
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤ –ª—é–±–æ–º –ø–æ—Ä—è–¥–∫–µ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏:
            # "0 —á–∞—Å–æ–≤", "0 –º–∏–Ω—É—Ç", "0 —Å–µ–∫—É–Ω–¥", "0 —á", "0 –º–∏–Ω", "0 —Å–µ–∫"
            # –¢–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã: "0, 0, 0" –∏–ª–∏ "00:00:00"
            
            # –£–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –∑–∞–ø—è—Ç—ã–º–∏ –∏ –ø—Ä–æ–±–µ–ª–∞–º–∏
            text = re.sub(r',?\s*0\s*(?:—á–∞—Å–æ–≤?|—á\.?)', '', text, flags=re.IGNORECASE)
            text = re.sub(r',?\s*0\s*(?:–º–∏–Ω—É—Ç?|–º–∏–Ω\.?)', '', text, flags=re.IGNORECASE)
            text = re.sub(r',?\s*0\s*(?:—Å–µ–∫—É–Ω–¥?|—Å–µ–∫\.?)', '', text, flags=re.IGNORECASE)
            
            # –£–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –±–µ–∑ –∑–∞–ø—è—Ç—ã—Ö (–≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ)
            text = re.sub(r'0\s*(?:—á–∞—Å–æ–≤?|—á\.?)\s*,?', '', text, flags=re.IGNORECASE)
            text = re.sub(r'0\s*(?:–º–∏–Ω—É—Ç?|–º–∏–Ω\.?)\s*,?', '', text, flags=re.IGNORECASE)
            text = re.sub(r'0\s*(?:—Å–µ–∫—É–Ω–¥?|—Å–µ–∫\.?)', '', text, flags=re.IGNORECASE)
            
            # –£–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∏–¥–∞ "00:00:00" –∏–ª–∏ "0:0:0"
            text = re.sub(r'\s*0+:0+:0+\s*', '', text)
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ "0, 0, 0" –∏–ª–∏ "0 0 0"
            text = re.sub(r'\s*0\s*,\s*0\s*,\s*0\s*', '', text)
            text = re.sub(r'\s*0\s+0\s+0\s*', '', text)
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ –∏ –ø—Ä–æ–±–µ–ª—ã
            text = re.sub(r',\s*,+', ',', text)  # –î–≤–æ–π–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ
            text = re.sub(r'^,\s*', '', text)  # –ó–∞–ø—è—Ç–∞—è –≤ –Ω–∞—á–∞–ª–µ
            text = re.sub(r'\s*,\s*$', '', text)  # –ó–∞–ø—è—Ç–∞—è –≤ –∫–æ–Ω—Ü–µ
            text = re.sub(r'\s+', ' ', text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
            
            return text.strip()
        
        for col in df.columns:
            col_norm = normalize_column_name(str(col))
            if '—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' in col_norm:
                df[col] = df[col].apply(clean_waiting_period)
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞—Ç –∫ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –≤–∏–¥—É (YYYY-MM-DD)
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∏–º–µ–Ω–∞–º
        for col in df.columns:
            col_norm = normalize_column_name(str(col))
            if '–¥–∞—Ç–∞' in col_norm and ('—Ç—Ç–Ω' in col_norm or '–º–∞—Ä—à—Ä—É—Ç' in col_norm):
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d').fillna('')
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ –∫ int (–µ—Å–ª–∏ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ)
        for col in df.columns:
            col_norm = normalize_column_name(str(col))
            if ('–Ω–æ–º–µ—Ä' in col_norm and '—Ç—Ç–Ω' in col_norm) or ('‚Ññ' in col_norm and '–º–∞—Ä—à—Ä—É—Ç' in col_norm):
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ int, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].apply(lambda x: int(x) if pd.notna(x) and x == int(x) else x)
                # –£–±–∏—Ä–∞–µ–º ".0" –∏–∑ –∫–æ–Ω—Ü–∞ —á–∏—Å–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, "123.0" -> "123")
                df[col] = df[col].astype(str).replace('nan', '').replace('<NA>', '').str.replace(r'\.0+$', '', regex=True)
        
        logger.debug(f"Docs Excel parsed successfully, rows: {len(df)}, columns: {list(df.columns)}")
        return df
    except Exception as e:
        logger.error(f"Failed to parse docs Excel: {e}")
        import traceback
        traceback.print_exc()
        raise


def detect_report_type(df: pd.DataFrame) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á—ë—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ç–∞–±–ª–∏—Ü—ã"""
    cols_lower = {str(c).lower(): c for c in df.columns}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ late-report: –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ "–û–ø–æ–∑–¥–∞–Ω–∏–µ"
    for col_lower in cols_lower:
        if '–æ–ø–æ–∑–¥–∞–Ω' in col_lower:
            return 'late'
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ docs-report: –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ "–ü—Ä–∏—á–∏–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¢–¢–ù" –∏–ª–∏ "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É"
    for col_lower in cols_lower:
        if '–ø—Ä–∏—á–∏–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏' in col_lower and '—Ç—Ç–Ω' in col_lower:
            return 'docs'
        if '—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' in col_lower:
            return 'docs'
    
    return 'unknown'


# Alias –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
determine_report_type = detect_report_type




def get_email_attachments(config: Dict) -> List[Tuple[int, int, str, bytes]]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ XLSX –≤–ª–æ–∂–µ–Ω–∏–π –∏–∑ –ø–∏—Å–µ–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback –¥–Ω–µ–π
    
    Returns:
        List[Tuple[uid, attachment_index, filename, file_data]]
    """
    if not config['imap_user'] or not config['imap_pass']:
        logger.error("IMAP credentials not set")
        return []
    
    attachments = []
    attachment_pattern = None
    # –ü—Ä–∏–º–µ–Ω—è–µ–º regex-—Ñ–∏–ª—å—Ç—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω –∏ –Ω–µ –ø—É—Å—Ç–æ–π
    if config.get('attachment_regex'):
        try:
            attachment_pattern = re.compile(config['attachment_regex'], re.IGNORECASE)
        except Exception as e:
            logger.warning(f"Invalid attachment_regex pattern: {e}, ignoring regex filter")
    
    try:
        with IMAPClient(config['imap_host'], port=993, ssl=True) as client:
            client.login(config['imap_user'], config['imap_pass'])
            mailbox = config.get('mailbox', 'INBOX')
            client.select_folder(mailbox)
            
            # –ü–æ–∏—Å–∫ –ø–∏—Å–µ–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback –¥–Ω–µ–π
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è "—Å–µ–≥–æ–¥–Ω—è"
            # –≠—Ç–æ –≤–∞–∂–Ω–æ, —Ç.–∫. DOCS –ø–∏—Å—å–º–∞ –ø—Ä–∏—Ö–æ–¥—è—Ç –æ–∫–æ–ª–æ 01:00 –ú–°–ö, –Ω–æ INTERNALDATE = –≤—á–µ—Ä–∞ –ø–æ UTC
            lookback_days = config.get('imap_lookback_days', 3)
            report_tz = config.get('report_tz', 'Europe/Moscow')
            max_uids = config.get('imap_max_uids', 500)
            
            try:
                tz = ZoneInfo(report_tz)
            except Exception as e:
                logger.warning(f"Invalid timezone {report_tz}, using UTC: {e}")
                tz = ZoneInfo('UTC')
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –≤ –º–æ—Å–∫–æ–≤—Å–∫–æ–º –≤—Ä–µ–º–µ–Ω–∏
            today_msk = datetime.now(tz).date()
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–∞—Ç—É –¥–ª—è –ø–æ–∏—Å–∫–∞ (—Å–µ–≥–æ–¥–Ω—è - lookback –¥–Ω–µ–π)
            since_date = today_msk - timedelta(days=lookback_days)
            since_str = since_date.strftime('%d-%b-%Y')
            
            logger.info(f"Computed SINCE: {since_str} (lookback {lookback_days}d, tz={report_tz}, today_msk={today_msk})")
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–∏—Å–µ–º —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
            messages = client.search(['SINCE', since_str])
            logger.info(f"Found {len(messages)} messages since {since_str}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ UID (–±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_uids)
            if len(messages) > max_uids:
                messages = sorted(messages)[-max_uids:]
                logger.info(f"Limited to last {max_uids} UIDs (total found: {len(messages)})")
            else:
                messages = sorted(messages)
                logger.info(f"Processing all {len(messages)} UIDs")
            
            attachment_index = 0
            for uid in messages:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º INTERNALDATE –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                    fetch_data = client.fetch([uid], ['RFC822', 'INTERNALDATE'])
                    msg_data = fetch_data[uid]
                    internaldate = msg_data.get(b'INTERNALDATE', b'unknown')
                    if isinstance(internaldate, bytes):
                        internaldate_str = internaldate.decode('utf-8', errors='replace')
                    else:
                        internaldate_str = str(internaldate)
                    
                    msg = email.message_from_bytes(msg_data[b'RFC822'])
                    
                    for part in msg.walk():
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ filename –≤ Content-Disposition (attachment –∏–ª–∏ inline)
                        content_disposition = part.get_content_disposition()
                        filename_raw = part.get_filename()
                        content_type = part.get_content_type()
                        
                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ MIME —Ñ–æ—Ä–º–∞—Ç–µ)
                        filename = decode_filename(filename_raw)
                        
                        # –ü—Ä–∏–Ω–∏–º–∞–µ–º –ª—é–±—É—é —á–∞—Å—Ç—å —Å filename, –¥–∞–∂–µ –µ—Å–ª–∏ content-disposition –Ω–µ "attachment"
                        # (inline —Ç–æ–∂–µ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–ª–æ–∂–µ–Ω–∏–µ)
                        if filename:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª Excel
                            if is_excel_file(filename, content_type):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º regex-—Ñ–∏–ª—å—Ç—Ä (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω –∏ filename –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω)
                                should_include = True
                                if attachment_pattern:
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫—Ä–∞–∫–æ–∑—è–±—Ä (—Å–∏–º–≤–æ–ª–æ–≤ –∑–∞–º–µ–Ω—ã –∏–ª–∏ –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö)
                                    # –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –∑–∞–º–µ–Ω—ã –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º regex
                                    has_encoding_issues = filename and ('' in filename or filename_raw != filename)
                                    if not has_encoding_issues:
                                        if not attachment_pattern.search(filename):
                                            should_include = False
                                            logger.debug(f"Attachment {filename} (UID {uid}) filtered out by regex")
                                    else:
                                        # –ï—Å–ª–∏ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º regex-–ø—Ä–æ–≤–µ—Ä–∫—É, –Ω–æ –≤–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª
                                        logger.info(f"Attachment filename contains encoding issues, skipping regex filter: {filename[:50]}")
                                
                                if should_include:
                                    try:
                                        file_data = part.get_payload(decode=True)
                                        if file_data:
                                            attachments.append((uid, attachment_index, filename or f"mail_{uid}.xlsx", file_data))
                                            logger.debug(f"Found Excel attachment: UID {uid}, INTERNALDATE {internaldate_str}, filename={filename[:50] if filename else 'N/A'}, index {attachment_index}, content-type: {content_type}")
                                            attachment_index += 1
                                    except Exception as e:
                                        logger.error(f"Failed to decode attachment {filename} (UID {uid}): {e}")
                except Exception as e:
                    logger.error(f"Error processing message {uid}: {e}")
    
    except Exception as e:
        logger.error(f"IMAP error: {e}")
        import traceback
        traceback.print_exc()
    
    return attachments


def normalize_column_name(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
    if not name or pd.isna(name):
        return ''
    name_str = str(name)
    # Lowercase
    name_str = name_str.lower()
    # Strip whitespace
    name_str = name_str.strip()
    # Replace NBSP (\xa0) with space
    name_str = name_str.replace('\xa0', ' ')
    # Replace multiple spaces with single space
    name_str = ' '.join(name_str.split())
    return name_str


# Alias –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
find_header_rows_docs = find_docs_header_row


def find_header_rows(file_data: bytes) -> int:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ Excel —Ñ–∞–π–ª–µ"""
    try:
        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        df_preview = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=None, nrows=10)
        
        # –ò—â–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É r, –≥–¥–µ –µ—Å—Ç—å —è—á–µ–π–∫–∞ —Å –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π "–æ–ø–æ–∑–¥–∞–Ω"
        for r in range(len(df_preview)):
            row_values = df_preview.iloc[r].astype(str).str.lower()
            if any('–æ–ø–æ–∑–¥–∞–Ω' in str(val) for val in row_values):
                logger.debug(f"Found '–æ–ø–æ–∑–¥–∞–Ω' in row {r}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É r+1 (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
                if r + 1 < len(df_preview):
                    next_row_values = df_preview.iloc[r + 1].astype(str)
                    # –°—á–∏—Ç–∞–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–µ NaN, –Ω–µ '', –Ω–µ 'nan')
                    non_empty_count = sum(1 for val in next_row_values if str(val).strip() and str(val).lower() != 'nan')
                    
                    if non_empty_count >= 3:
                        logger.debug(f"Row {r+1} has {non_empty_count} non-empty values, using header=[{r}, {r+1}]")
                        return r
                    else:
                        logger.debug(f"Row {r+1} has only {non_empty_count} non-empty values, using header={r}")
                        return r
                else:
                    logger.debug(f"No row {r+1}, using header={r}")
                    return r
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0 (fallback)
        logger.warning("'–æ–ø–æ–∑–¥–∞–Ω' not found in first 10 rows, using header=0")
        return 0
    except Exception as e:
        logger.warning(f"Failed to find header rows: {e}, using header=0")
        return 0


def parse_excel(file_data: bytes) -> pd.DataFrame:
    """–ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å—Ç—Ä–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        header_row = find_header_rows(file_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ –≤—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        df_preview = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=None, nrows=header_row + 2)
        if header_row + 1 < len(df_preview):
            next_row_values = df_preview.iloc[header_row + 1].astype(str)
            non_empty_count = sum(1 for val in next_row_values if str(val).strip() and str(val).lower() != 'nan')
            
            if non_empty_count >= 3:
                header = [header_row, header_row + 1]
            else:
                header = header_row
        else:
            header = header_row
        
        logger.debug(f"Reading Excel with header={header}")
        
        # –ß–∏—Ç–∞–µ–º Excel —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        df = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=header)
        
        # –°–ø–ª—é—â–∏–≤–∞–Ω–∏–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —à–∞–ø–∫–∏
        if isinstance(df.columns, pd.MultiIndex):
            # Forward fill –ø–æ –≤–µ—Ä—Ö–Ω–µ–º—É —É—Ä–æ–≤–Ω—é –¥–ª—è merged —è—á–µ–µ–∫ (NaN/Unnamed)
            columns_df = pd.DataFrame(list(df.columns))
            columns_df[0] = columns_df[0].replace('', pd.NA).ffill()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º MultiIndex —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            new_columns = list(zip(columns_df[0], columns_df[1]))
            df.columns = pd.MultiIndex.from_tuples(new_columns)
            
            # Flatten –∫–æ–ª–æ–Ω–æ–∫
            flattened_columns = []
            for col_tuple in df.columns:
                lvl0 = str(col_tuple[0]) if pd.notna(col_tuple[0]) else ''
                lvl1 = str(col_tuple[1]) if pd.notna(col_tuple[1]) else ''
                
                lvl0 = lvl0.strip()
                lvl1 = lvl1.strip()
                
                # –ü—Ä–∞–≤–∏–ª–∞ flatten:
                # - –µ—Å–ª–∏ lvl1 –ø—É—Å—Ç–æ–π/NaN/Unnamed -> col = lvl0
                # - elif lvl0 –ø—É—Å—Ç–æ–π -> col = lvl1
                # - else col = f"{lvl0} - {lvl1}"
                if not lvl1 or lvl1 == '' or lvl1.lower().startswith('unnamed'):
                    flattened_columns.append(lvl0)
                elif not lvl0 or lvl0 == '':
                    flattened_columns.append(lvl1)
                else:
                    flattened_columns.append(f"{lvl0} - {lvl1}")
            
            df.columns = flattened_columns
        
        # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫ (strip whitespace)
        df.columns = [str(c).strip() for c in df.columns]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º file_data –∏ header –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö DataFrame –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        df.attrs['_file_data'] = file_data
        df.attrs['_header_rows'] = header if isinstance(header, list) else [header]
        
        logger.debug(f"Excel parsed successfully with header={header}, columns: {list(df.columns)[:10]}...")
        return df
    except Exception as e:
        logger.error(f"Failed to parse Excel: {e}")
        import traceback
        traceback.print_exc()
        raise


def find_columns(df: pd.DataFrame) -> Dict[str, str]:
    """–ü–æ–∏—Å–∫ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º"""
    cols_map = {}
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
    normalized_cols = {normalize_column_name(str(c)): str(c) for c in df.columns}
    
    # –ü–æ–∏—Å–∫ delay –∫–æ–ª–æ–Ω–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è): —Å–æ–¥–µ—Ä–∂–∏—Ç "–æ–ø–æ–∑–¥–∞–Ω"
    for col_norm, col_orig in normalized_cols.items():
        if '–æ–ø–æ–∑–¥–∞–Ω' in col_norm:
            cols_map['delay'] = col_orig
            logger.debug(f"Found delay column: '{col_orig}' (normalized: '{col_norm}')")
            break
    
    # –ü–æ–∏—Å–∫ FIO –∫–æ–ª–æ–Ω–∫–∏: —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ñ–∏–æ"
    for col_norm, col_orig in normalized_cols.items():
        if '—Ñ–∏–æ' in col_norm:
            cols_map['driver_name'] = col_orig
            break
    
    # –ü–æ–∏—Å–∫ –≥–æ—Å–Ω–æ–º–µ—Ä–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç "–≥–æ—Å"
    for col_norm, col_orig in normalized_cols.items():
        if '–≥–æ—Å' in col_norm:
            cols_map['plate'] = col_orig
            break
    
    # –ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–∞ (–¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å "—Ç–∏–ø–æ–≤–æ–π" –ò "–Ω–∞–∏–º–µ–Ω")
    for col_norm, col_orig in normalized_cols.items():
        if '—Ç–∏–ø–æ–≤–æ–π' in col_norm and '–Ω–∞–∏–º–µ–Ω' in col_norm:
            cols_map['route_name'] = col_orig
            break
    
    # –ü–æ–∏—Å–∫ –ø–ª–∞–Ω–æ–≤–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å "–ø–ª–∞–Ω–æ–≤" –ò "–ø–æ–¥–∞—á")
    for col_norm, col_orig in normalized_cols.items():
        if '–ø–ª–∞–Ω–æ–≤' in col_norm and '–ø–æ–¥–∞—á' in col_norm:
            cols_map['planned_time'] = col_orig
            break
    
    # –ü–æ–∏—Å–∫ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è: —Å–æ–¥–µ—Ä–∂–∏—Ç "–≤—Ä–µ–º—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è"
    for col_norm, col_orig in normalized_cols.items():
        if '–≤—Ä–µ–º—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è' in col_norm:
            cols_map['assigned_time'] = col_orig
            break
    
    # –ï—Å–ª–∏ delay –∫–æ–ª–æ–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –ª–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ raw
    if 'delay' not in cols_map:
        header_rows = df.attrs.get('_header_rows', 'unknown')
        logger.warning(f"Delay column not found. Header rows: {header_rows}, Columns after flatten: {list(df.columns)}")
        logger.warning(f"Normalized columns: {list(normalized_cols.keys())}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ raw –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        try:
            file_data = df.attrs.get('_file_data', None)
            if file_data is None:
                # –ï—Å–ª–∏ file_data –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                logger.warning("Cannot access raw file data for logging")
            else:
                df_preview = pd.read_excel(io.BytesIO(file_data), engine="openpyxl", header=None, nrows=5)
                logger.warning(f"First 5 raw rows:\n{df_preview.head().to_string()}")
        except Exception as e:
            logger.warning(f"Failed to log raw rows: {e}")
    
    return cols_map


def extract_late_records(df: pd.DataFrame) -> List[Dict]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –æ–ø–æ–∑–¥–∞–Ω–∏—è–º–∏"""
    cols = find_columns(df)
    
    if 'delay' not in cols:
        logger.error("Delay column not found")
        return []
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–ª—è delay –∫–æ–ª–æ–Ω–∫–∏
    delay_series = pd.to_numeric(df[cols['delay']], errors="coerce").fillna(0).astype(int)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–ø–æ–∑–¥–∞–≤—à–∏—Ö (delay > 0)
    df_filtered = df[delay_series > 0].copy()
    
    if len(df_filtered) == 0:
        logger.info("No late records found (all delays <= 0)")
        return []
    
    records = []
    for _, row in df_filtered.iterrows():
        delay = int(pd.to_numeric(row[cols['delay']], errors="coerce") or 0)
        if delay <= 0:
            continue
        
        record = {
            'delay_minutes': delay,
            'driver_name': str(row.get(cols.get('driver_name', ''), '‚Äî')).strip() if cols.get('driver_name') else '‚Äî',
            'plate_number': str(row.get(cols.get('plate', ''), '‚Äî')).strip() if cols.get('plate') else '‚Äî',
            'route_name': str(row.get(cols.get('route_name', ''), '‚Äî')).strip() if cols.get('route_name') else '‚Äî',
            'planned_time': str(row.get(cols.get('planned_time', ''), '‚Äî')).strip() if cols.get('planned_time') else '‚Äî',
            'assigned_time': str(row.get(cols.get('assigned_time', ''), '‚Äî')).strip() if cols.get('assigned_time') else '‚Äî',
        }
        records.append(record)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–ø–æ–∑–¥–∞–Ω–∏—é –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    records.sort(key=lambda x: x['delay_minutes'], reverse=True)
    
    logger.info(f"Extracted {len(records)} late records from Excel")
    return records


def get_delay_emoji(delay: int) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏ –¥–ª—è –æ–ø–æ–∑–¥–∞–Ω–∏—è"""
    if delay >= 21:
        return 'üî¥'
    elif delay >= 11:
        return 'üü°'
    else:
        return 'üü¢'


def generate_png_table_docs(df: pd.DataFrame, output_path: str):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PNG —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è docs-report (–æ—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)"""
    if len(df) == 0:
        return False
    
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    columns_to_remove = []
    for col in df.columns:
        col_lower = normalize_column_name(str(col))
        # –£–±–∏—Ä–∞–µ–º: "–ü–ª–æ—â–∞–¥–∫–∞", "–ù–æ–º–µ—Ä –º–∞—Ä—à—Ä—É—Ç–∞", "–ì–æ—Å. ‚Ññ –∞/–º", "–î–∞—Ç–∞ –º–∞—Ä—à—Ä—É—Ç–∞"
        if any(word in col_lower for word in ['–ø–ª–æ—â–∞–¥–∫', '–Ω–æ–º–µ—Ä –º–∞—Ä—à—Ä—É—Ç', '–≥–æ—Å', '‚Ññ –∞/–º', '–≥–æ—Å–Ω–æ–º–µ—Ä']):
            if '–Ω–æ–º–µ—Ä' in col_lower or '‚Ññ' in col_lower:
                columns_to_remove.append(col)
            elif '–ø–ª–æ—â–∞–¥–∫' in col_lower or ('–≥–æ—Å' in col_lower and '‚Ññ' in col_lower):
                columns_to_remove.append(col)
        # –£–±–∏—Ä–∞–µ–º "–î–∞—Ç–∞ –º–∞—Ä—à—Ä—É—Ç–∞" (–Ω–æ –Ω–µ "–î–∞—Ç–∞ –¢–¢–ù")
        elif '–¥–∞—Ç–∞' in col_lower and '–º–∞—Ä—à—Ä—É—Ç' in col_lower and '—Ç—Ç–Ω' not in col_lower:
            columns_to_remove.append(col)
        # –£–±–∏—Ä–∞–µ–º "–ù–æ–º–µ—Ä –º–∞—Ä—à—Ä—É—Ç–∞" / "‚Ññ –º–∞—Ä—à—Ä—É—Ç–∞" (–Ω–æ –Ω–µ "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞")
        elif ('–Ω–æ–º–µ—Ä' in col_lower or '‚Ññ' in col_lower) and '–º–∞—Ä—à—Ä—É—Ç' in col_lower and '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω' not in col_lower:
            columns_to_remove.append(col)
    
    if columns_to_remove:
        df = df.drop(columns=columns_to_remove, errors='ignore')
        logger.info(f"Removed columns from docs-report: {columns_to_remove}")
    
    if len(df.columns) == 0:
        logger.warning("No columns left after filtering")
        return False
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–±–ª–∏—Ü—ã (–∫–∞–∫ –≤ late-report –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è)
    cell_padding = 10  # –ö–∞–∫ –≤ generate_png_table
    row_height = 50  # –ö–∞–∫ –≤ generate_png_table
    header_height = 50  # –ö–∞–∫ –≤ generate_png_table
    font_size = 11  # –£–≤–µ–ª–∏—á–µ–Ω –Ω–∞ 1 –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
    # –£–∑–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞): 120-150
    # –°—Ä–µ–¥–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–§–ò–û, –∫–æ–º–ø–∞–Ω–∏—è): 200-250
    # –®–∏—Ä–æ–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø—É–Ω–∫—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–∏—á–∏–Ω–∞): 300-400
    num_cols = len(df.columns)
    col_widths = []
    
    for col in df.columns:
        col_lower = normalize_column_name(str(col))
        # –û—á–µ–Ω—å —É–∑–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫–æ–¥—ã, –¥–∞—Ç—ã)
        if any(word in col_lower for word in ['–∫–æ–¥ –ø–æ–ª—É—á–∞—Ç–µ–ª', '–∫–æ–¥']):
            col_widths.append(90)  # –£–∑–∫–∞—è –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∫–æ–¥–∞
        # –£–∑–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞)
        elif any(word in col_lower for word in ['–¥–∞—Ç–∞', '—Ç—Ç–Ω']):
            col_widths.append(110)
        # –°—Ä–µ–¥–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è - –Ω–µ–º–Ω–æ–≥–æ –ø–æ—É–∂–µ)
        elif any(word in col_lower for word in ['—Ñ–∏–æ', '–≤–æ–¥–∏—Ç–µ–ª']):
            col_widths.append(160)  # –ë—ã–ª–æ 170, —Å—Ç–∞–ª–æ 160 (–µ—â–µ –ø–æ—É–∂–µ)
        # –£–∑–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —É–∑–∫–∞—è)
        elif any(word in col_lower for word in ['—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤']):
            col_widths.append(140)  # –£–∑–∫–∞—è –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        # –°—Ä–µ–¥–Ω–∏–µ-—à–∏—Ä–æ–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫–æ–º–ø–∞–Ω–∏—è –ø–æ–ª—É—á–∞—Ç–µ–ª—è)
        elif any(word in col_lower for word in ['–∫–æ–º–ø–∞–Ω–∏', '–ø–æ–ª—É—á–∞—Ç–µ–ª']):
            col_widths.append(200)
        # –®–∏—Ä–æ–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø—É–Ω–∫—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–∏—á–∏–Ω–∞ - –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞)
        elif any(word in col_lower for word in ['–ø—É–Ω–∫—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω']):
            col_widths.append(300)  # –ë—ã–ª–æ 280, —Å—Ç–∞–ª–æ 300 (–±–æ–ª—å—à–µ –º–µ—Å—Ç–∞)
        # –û—á–µ–Ω—å —à–∏—Ä–æ–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø—Ä–∏—á–∏–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¢–¢–ù - –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞)
        elif any(word in col_lower for word in ['–ø—Ä–∏—á–∏–Ω–∞']):
            col_widths.append(320)  # –ë—ã–ª–æ 300, —Å—Ç–∞–ª–æ 320 (–µ—â–µ –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞)
        # –û—á–µ–Ω—å —à–∏—Ä–æ–∫–∏–µ (–∞–¥—Ä–µ—Å–∞, –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è)
        elif any(word in col_lower for word in ['–∞–¥—Ä–µ—Å', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω']):
            col_widths.append(340)  # –ë—ã–ª–æ 320, —Å—Ç–∞–ª–æ 340 (–±–æ–ª—å—à–µ –º–µ—Å—Ç–∞)
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Å—Ä–µ–¥–Ω—è—è —à–∏—Ä–∏–Ω–∞
        else:
            col_widths.append(180)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —à–∏—Ä–∏–Ω—ã, —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ ~1200 (–∫–∞–∫ –≤ late-report)
    total_width = sum(col_widths)
    target_width = 1200
    if total_width > target_width:
        scale = target_width / total_width
        col_widths = [int(w * scale) for w in col_widths]
    elif total_width < 800:
        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —É–∑–∫–æ, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º
        scale = 800 / total_width
        col_widths = [int(w * scale) for w in col_widths]
    
    table_width = sum(col_widths) + (num_cols + 1) * 2  # + –æ—Ç—Å—Ç—É–ø—ã
    
    num_rows = len(df) + 1  # +1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    table_height = header_height + num_rows * row_height
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = Image.new('RGB', (table_width, table_height), color='white')
    draw = ImageDraw.Draw(img)
    
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", font_size)
        font_bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
    except:
        font = ImageFont.load_default()
        font_bold = font
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = list(df.columns)
    
    x = 2
    y = 2
    
    # –†–∏—Å–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    for i, header in enumerate(headers):
        col_w = col_widths[i]
        cell_rect = [x, y, x + col_w, y + header_height]
        draw.rectangle(cell_rect, outline='black', width=2)
        draw.rectangle([cell_rect[0]+1, cell_rect[1]+1, cell_rect[2]-1, cell_rect[3]-1], fill='#f0f0f0')
        
        # –¢–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ (wrap –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
        header_text = str(header)
        col_lower = normalize_column_name(str(header))
        
        # –î–ª—è "–ö–æ–¥ –ø–æ–ª—É—á–∞—Ç–µ–ª—è" - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –≤ –¥–≤–µ —Å—Ç—Ä–æ–∫–∏
        if '–∫–æ–¥ –ø–æ–ª—É—á–∞—Ç–µ–ª' in col_lower:
            # –†–∞–∑–±–∏–≤–∞–µ–º "–ö–æ–¥ –ø–æ–ª—É—á–∞—Ç–µ–ª—è" –Ω–∞ –¥–≤–µ —Å—Ç—Ä–æ–∫–∏
            words = header_text.split()
            if len(words) >= 2:
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ "–ö–æ–¥" –∏ "–ø–æ–ª—É—á–∞—Ç–µ–ª—è"
                wrapped_lines = [words[0], ' '.join(words[1:])]
            else:
                # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ, –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –ø–æ–ø–æ–ª–∞–º
                mid = len(header_text) // 2
                wrapped_lines = [header_text[:mid], header_text[mid:]]
        # –î–ª—è "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É" - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –≤ —Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏
        elif '—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' in col_lower:
            # –†–∞–∑–±–∏–≤–∞–µ–º "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É" –Ω–∞ —Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏
            words = header_text.split()
            if len(words) >= 3:
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è", "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ", "–º–∞—Ä—à—Ä—É—Ç—É"
                if len(words) == 6:  # "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É"
                    wrapped_lines = ['–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ', '–º–∞—Ä—à—Ä—É—Ç—É']
                elif len(words) == 5:  # "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–∞—Ä—à—Ä—É—Ç—É" (–≤–∞—Ä–∏–∞–Ω—Ç)
                    wrapped_lines = ['–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ', '–º–∞—Ä—à—Ä—É—Ç—É']
                else:
                    # –û–±—â–∏–π —Å–ª—É—á–∞–π: –¥–µ–ª–∏–º –Ω–∞ 3 —á–∞—Å—Ç–∏
                    part_size = len(words) // 3
                    wrapped_lines = [
                        ' '.join(words[:part_size]),
                        ' '.join(words[part_size:2*part_size]),
                        ' '.join(words[2*part_size:])
                    ]
            else:
                # –ï—Å–ª–∏ –º–∞–ª–æ —Å–ª–æ–≤, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                part_size = len(header_text) // 3
                wrapped_lines = [header_text[:part_size], header_text[part_size:2*part_size], header_text[2*part_size:]]
        else:
            # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            wrap_width = max(12, col_w // 7)  # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞
            wrapped_lines = textwrap.wrap(header_text, width=wrap_width)
        
        text_y = cell_rect[1] + cell_padding
        line_spacing = 11  # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫–∞–º–∏
        for line in wrapped_lines[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            draw.text((cell_rect[0] + cell_padding, text_y), line, fill='black', font=font_bold)
            text_y += line_spacing
        
        x += col_w + 2
    
    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏ —Å—Ç—Ä–æ–∫
    waiting_period_col = None
    for col in df.columns:
        col_norm = normalize_column_name(str(col))
        if '—Å—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' in col_norm:
            waiting_period_col = col
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –¥–∞—Ç—É –ø–æ –º–æ—Å–∫–æ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏
    today_msk = datetime.now(ZoneInfo('Europe/Moscow')).date()
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∞–∑–Ω–∏—Ü—ã –≤ –¥–Ω—è—Ö –º–µ–∂–¥—É "–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" –∏ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–π –¥–∞—Ç–æ–π
    def calculate_days_diff(row):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–Ω—è—Ö –º–µ–∂–¥—É '–°—Ä–æ–∫ –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' –∏ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–π –¥–∞—Ç–æ–π (–ú–°–ö)"""
        if not waiting_period_col:
            return None
        
        waiting_date_str = str(row.get(waiting_period_col, '')) if pd.notna(row.get(waiting_period_col)) else ''
        
        if not waiting_date_str:
            return None
        
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–µ–∑ dayfirst (–¥–ª—è YYYY-MM-DD), –ø–æ—Ç–æ–º —Å dayfirst (–¥–ª—è DD.MM.YYYY)
            waiting_date = pd.to_datetime(waiting_date_str, errors='coerce', dayfirst=False)
            if pd.isna(waiting_date):
                waiting_date = pd.to_datetime(waiting_date_str, errors='coerce', dayfirst=True)
            
            if pd.isna(waiting_date):
                return None
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ date –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            waiting_date_only = waiting_date.date()
            
            # –†–∞–∑–Ω–∏—Ü–∞ –≤ –¥–Ω—è—Ö (waiting_date - today_msk)
            # –ï—Å–ª–∏ waiting_date –≤ –±—É–¥—É—â–µ–º, —Ä–∞–∑–Ω–∏—Ü–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è
            # –ï—Å–ª–∏ waiting_date –≤ –ø—Ä–æ—à–ª–æ–º, —Ä–∞–∑–Ω–∏—Ü–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è
            diff = (waiting_date_only - today_msk).days
            return diff
        except:
            return None
    
    # –î–∞–Ω–Ω—ã–µ
    y = header_height + 2
    for idx, row in df.iterrows():
        x = 2
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–Ω—è—Ö –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏
        days_diff = calculate_days_diff(row)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç —Ñ–æ–Ω–∞ —Å—Ç—Ä–æ–∫–∏ (–±–æ–ª–µ–µ –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞)
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞: < 2 –¥–Ω–µ–π ‚Üí –∫—Ä–∞—Å–Ω–∞—è, 2-4 –¥–Ω—è ‚Üí –æ—Ä–∞–Ω–∂–µ–≤–∞—è, >= 4 –¥–Ω–µ–π ‚Üí –±–µ–ª–∞—è
        if days_diff is not None:
            if days_diff >= 0 and days_diff < 2:
                row_bg_color = '#ff8888'  # –ß—É—Ç—å –∫—Ä–∞—Å–Ω–µ–µ (< 2 –¥–Ω–µ–π)
            elif days_diff >= 2 and days_diff < 4:
                row_bg_color = '#ffd4aa'  # –ß—É—Ç—å –ø–æ–±–ª–µ–¥–Ω–µ–µ (2-4 –¥–Ω—è)
            else:
                row_bg_color = '#ffffff'  # –ë–µ–ª—ã–π (>= 4 –¥–Ω–µ–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞)
        else:
            row_bg_color = '#ffffff'  # –ë–µ–ª—ã–π (–Ω–µ—Ç –¥–∞—Ç –∏–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã)
        
        for i, col_name in enumerate(headers):
            col_w = col_widths[i]
            cell_rect = [x, y, x + col_w, y + row_height]
            draw.rectangle(cell_rect, outline='black', width=1)
            
            # –ó–∞–ª–∏–≤–∞–µ–º —Ñ–æ–Ω —è—á–µ–π–∫–∏ —Ü–≤–µ—Ç–æ–º —Å—Ç—Ä–æ–∫–∏
            draw.rectangle([cell_rect[0]+1, cell_rect[1]+1, cell_rect[2]-1, cell_rect[3]-1], fill=row_bg_color)
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —è—á–µ–π–∫–∏
            cell_value = str(row[col_name]) if pd.notna(row[col_name]) else ''
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∏—Ä–∏–Ω—É –ø–µ—Ä–µ–Ω–æ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–µ–∂–∏–º)
            col_lower = normalize_column_name(str(col_name))
            if any(word in col_lower for word in ['–ø—É–Ω–∫—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω', '–∞–¥—Ä–µ—Å', '–ø—Ä–∏—á–∏–Ω–∞', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω', '–∫–æ–º–ø–∞–Ω–∏']):
                wrap_width = max(16, col_w // 9)  # –®–∏—Ä–æ–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ - –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å
            elif any(word in col_lower for word in ['—Ñ–∏–æ', '–≤–æ–¥–∏—Ç–µ–ª']):
                wrap_width = max(14, col_w // 8)  # –°—Ä–µ–¥–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            else:
                wrap_width = max(12, col_w // 7)  # –£–∑–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º textwrap —Å break_long_words=False, —á—Ç–æ–±—ã —Å–ª–æ–≤–∞ –Ω–µ —Ä–∞–∑–±–∏–≤–∞–ª–∏—Å—å –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
            wrapped_lines = textwrap.wrap(cell_value, width=wrap_width, break_long_words=False, break_on_hyphens=False)
            
            if not wrapped_lines:
                wrapped_lines = ['']
            
            # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
            line_height = 11  # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫–∞–º–∏
            total_text_height = len(wrapped_lines) * line_height + (len(wrapped_lines) - 1) * 1
            text_y_start = cell_rect[1] + (row_height - total_text_height) // 2
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–∫–∏
            max_chars = min(45, col_w // 6)
            for line in wrapped_lines[:4]:  # –ú–∞–∫—Å–∏–º—É–º 4 —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
                line_text = line[:max_chars] if len(line) > max_chars else line
                draw.text((cell_rect[0] + cell_padding, text_y_start), line_text, fill='black', font=font)
                text_y_start += line_height
            
            x += col_w + 2
        
        y += row_height
    
    img.save(output_path)
    logger.info(f"Generated docs-report PNG table: {output_path}")
    return True


def generate_png_table(records: List[Dict], output_path: str):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PNG —Ç–∞–±–ª–∏—Ü—ã —Å –æ–ø–æ–∑–¥–∞–≤—à–∏–º–∏"""
    if not records:
        return False
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–±–ª–∏—Ü—ã
    cell_padding = 10
    row_height = 50  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 40 –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    # –®–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫: —É–≤–µ–ª–∏—á–µ–Ω–∞ 3-—è –∫–æ–ª–æ–Ω–∫–∞ "–í—Ä–µ–º—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è..." —Å 150 –¥–æ 180
    col_widths = [250, 150, 180, 100, 200, 120]  # –®–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫
    header_height = 50
    
    num_rows = len(records) + 1  # +1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    table_width = sum(col_widths) + (len(col_widths) + 1) * 2  # +–≥—Ä–∞–Ω–∏—Ü—ã
    table_height = header_height + num_rows * row_height
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = Image.new('RGB', (table_width, table_height), color='white')
    draw = ImageDraw.Draw(img)
    
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
        font_bold = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 12)
    except:
        font = ImageFont.load_default()
        font_bold = font
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = ['–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞', '–ü–ª–∞–Ω–æ–≤–æ–µ –≤—Ä–µ–º—è –ø–æ–¥–∞—á–∏', '–í—Ä–µ–º—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∞/–º –Ω–∞ –º–∞—Ä—à—Ä—É—Ç (—Ñ–∞–∫—Ç)', 
               '–û–ø–æ–∑–¥–∞–Ω–∏–µ, –º–∏–Ω.', '–§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è', '–ì–æ—Å. ‚Ññ']
    
    x = 2
    y = 2
    
    # –†–∏—Å–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    for i, header in enumerate(headers):
        cell_rect = [x, y, x + col_widths[i], y + header_height]
        draw.rectangle(cell_rect, outline='black', width=2)
        draw.rectangle([cell_rect[0]+1, cell_rect[1]+1, cell_rect[2]-1, cell_rect[3]-1], fill='#f0f0f0')
        
        # –¢–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏
        words = header.split()
        text_lines = []
        current_line = []
        for word in words:
            test_line = ' '.join(current_line + [word])
            bbox = draw.textbbox((0, 0), test_line, font=font_bold)
            if bbox[2] - bbox[0] <= col_widths[i] - 2 * cell_padding:
                current_line.append(word)
            else:
                if current_line:
                    text_lines.append(' '.join(current_line))
                current_line = [word]
        if current_line:
            text_lines.append(' '.join(current_line))
        
        text_y = cell_rect[1] + cell_padding
        for line in text_lines[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Å—Ç—Ä–æ–∫–∏
            draw.text((cell_rect[0] + cell_padding, text_y), line, fill='black', font=font_bold)
            text_y += 15
        
        x += col_widths[i] + 2
    
    # –î–∞–Ω–Ω—ã–µ
    y = header_height + 2
    for record in records:
        x = 2
        row_data = [
            record['route_name'],
            record['planned_time'],
            record['assigned_time'],
            str(record['delay_minutes']),
            record['driver_name'],
            record['plate_number'],
        ]
        
        for i, cell_text in enumerate(row_data):
            cell_rect = [x, y, x + col_widths[i], y + row_height]
            draw.rectangle(cell_rect, outline='black', width=1)
            
            # –û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            text = str(cell_text)[:40] + ('...' if len(str(cell_text)) > 40 else '')
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤—ã—Å–æ—Ç—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
            text_bbox = draw.textbbox((0, 0), text, font=font)
            text_height = text_bbox[3] - text_bbox[1]
            text_width = text_bbox[2] - text_bbox[0]
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ
            if '\n' in text:
                lines = text.split('\n')
                line_height = text_height
                total_text_height = len(lines) * line_height + (len(lines) - 1) * 5  # + –º–µ–∂—Å—Ç—Ä–æ—á–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                text_y_start = cell_rect[1] + (row_height - total_text_height) // 2
                for line in lines:
                    if i == 0:  # –ü–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞" - –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –ª–µ–≤–æ–º—É –∫—Ä–∞—é
                        text_x = cell_rect[0] + cell_padding
                    elif i == 2:  # 3-—è –∫–æ–ª–æ–Ω–∫–∞ "–í—Ä–µ–º—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è..." - —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
                        line_bbox = draw.textbbox((0, 0), line, font=font)
                        line_width = line_bbox[2] - line_bbox[0]
                        text_x = cell_rect[0] + (col_widths[i] - line_width) // 2
                    else:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ —Ü–µ–Ω—Ç—Ä—É –¥–ª—è —á–∏—Å–µ–ª
                        line_bbox = draw.textbbox((0, 0), line, font=font)
                        line_width = line_bbox[2] - line_bbox[0]
                        text_x = cell_rect[0] + (col_widths[i] - line_width) // 2
                    draw.text((text_x, text_y_start), line, fill='black', font=font)
                    text_y_start += line_height + 5
            else:
                # –û–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç - —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏
                cell_height = row_height
                text_y = cell_rect[1] + (cell_height - text_height) // 2
                
                if i == 0:  # –ü–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞" - –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –ª–µ–≤–æ–º—É –∫—Ä–∞—é
                    text_x = cell_rect[0] + cell_padding
                else:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏
                    text_x = cell_rect[0] + (col_widths[i] - text_width) // 2
                
                draw.text((text_x, text_y), text, fill='black', font=font)
            
            x += col_widths[i] + 2
        
        y += row_height
    
    img.save(output_path)
    logger.info(f"Generated PNG table: {output_path}")
    return True


def send_telegram_photo(config: Dict, photo_path: str, caption: str, topic_id: Optional[int] = None):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ç–æ –≤ Telegram –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é —Ç–µ–º—É"""
    if not config['tg_token'] or not config['tg_chat_id']:
        logger.error("Telegram credentials not set")
        return False
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π topic_id –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    if topic_id is None:
        topic_id = config.get('tg_topic_id_late') or config.get('tg_topic_id')
    
    if config.get('dry_run', False):
        logger.info(f"[DRY_RUN] Would send photo to Telegram: chat {config['tg_chat_id']}, topic {topic_id}, caption length: {len(caption)}")
        return True
    
    url = f"https://api.telegram.org/bot{config['tg_token']}/sendPhoto"
    
    data = {
        'chat_id': config['tg_chat_id'],
        'caption': caption[:1024],  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
    }
    
    if topic_id:
        data['message_thread_id'] = int(topic_id)
    
    try:
        with open(photo_path, 'rb') as photo:
            files = {'photo': photo}
            response = requests.post(url, data=data, files=files, timeout=30)
            response.raise_for_status()
            logger.info(f"Photo sent successfully to topic {topic_id}")
            return True
    except Exception as e:
        logger.error(f"Failed to send photo: {e}")
        return False


def send_telegram_text(config: Dict, text: str):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ Telegram"""
    if not config['tg_token'] or not config['tg_chat_id']:
        return False
    
    if config.get('dry_run', False):
        logger.info(f"[DRY_RUN] Would send text to Telegram: {text[:100]}...")
        return True
    
    url = f"https://api.telegram.org/bot{config['tg_token']}/sendMessage"
    
    data = {
        'chat_id': config['tg_chat_id'],
        'text': text[:4096],
    }
    
    if config['tg_topic_id']:
        data['message_thread_id'] = int(config['tg_topic_id'])
    
    try:
        response = requests.post(url, json=data, timeout=30)
        response.raise_for_status()
        return True
    except Exception as e:
        logger.error(f"Failed to send text: {e}")
        return False


def send_telegram_message(config: Dict, text: str, topic_id: Optional[int] = None):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é —Ç–µ–º—É"""
    if not config['tg_token'] or not config['tg_chat_id']:
        logger.error("Telegram credentials not set")
        return False
    
    if config.get('dry_run', False):
        logger.info(f"[DRY_RUN] Would send message to Telegram: topic {topic_id}, text: {text[:100]}...")
        return True
    
    url = f"https://api.telegram.org/bot{config['tg_token']}/sendMessage"
    
    data = {
        'chat_id': config['tg_chat_id'],
        'text': text[:4096],
    }
    
    if topic_id:
        data['message_thread_id'] = int(topic_id)
    
    try:
        response = requests.post(url, json=data, timeout=30)
        response.raise_for_status()
        logger.info(f"Message sent successfully to topic {topic_id}")
        return True
    except Exception as e:
        logger.error(f"Failed to send message: {e}")
        return False


def format_caption(records: List[Dict]) -> str:
    """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏ —Å –æ–ø–æ–∑–¥–∞–≤—à–∏–º–∏"""
    lines = []
    for record in records:
        emoji = get_delay_emoji(record['delay_minutes'])
        # –§–æ—Ä–º–∞—Ç: "üü° –ö–æ–±–∏–ª–æ–≤ –®.–ê. ‚Äî 16" (–±–µ–∑ "+" –∏ "–º–∏–Ω", –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ —ç–º–æ–¥–∑–∏)
        line = f"{emoji} {record['driver_name']} ‚Äî {record['delay_minutes']}"
        lines.append(line)
    
    caption = '\n'.join(lines)
    if len(caption) > 1024:
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 1020 —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã –≤–ª–µ–∑–ª–æ "..." 
        caption = caption[:1020] + '...'
    
    return caption


def mark_email_seen(config: Dict, uid: int):
    """–ü–æ–º–µ—Ç–∏—Ç—å –ø–∏—Å—å–º–æ –∫–∞–∫ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–µ"""
    try:
        with IMAPClient(config['imap_host'], port=993, ssl=True) as client:
            client.login(config['imap_user'], config['imap_pass'])
            client.select_folder('INBOX')
            client.set_flags([uid], [b'\\Seen'])
            logger.info(f"Marked message {uid} as seen")
    except Exception as e:
        logger.error(f"Failed to mark message as seen: {e}")


def get_file_hash(file_data: bytes) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–µ–π"""
    return hashlib.sha256(file_data).hexdigest()


def process_docs_report(config: Dict, attachments: List[Tuple[int, int, str, bytes]], processed_keys: Dict[str, float]) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ docs-report (–æ—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)"""
    if not config['run_docs_report']:
        logger.info("Docs-report disabled (RUN_DOCS_REPORT=0)")
        return
    
    logger.info("Processing docs-report (–æ—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)")
    
    all_docs_dfs = []
    
    for uid, att_index, filename, file_data in attachments:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –≤–ª–æ–∂–µ–Ω–∏—è
        file_hash = hashlib.sha256(file_data).hexdigest()
        attachment_key = f"{uid}:{att_index}:{file_hash}"
        
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ Excel –¥–ª—è docs-report
            df = parse_docs_excel(file_data)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç—á—ë—Ç–∞
            report_type = detect_report_type(df)
            
            if report_type != 'docs':
                logger.debug(f"File {filename} (UID {uid}) is not docs-report, skipping")
                continue
            
            # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ –§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è
            fio_col = find_fio_column(df)
            
            if not fio_col:
                # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                # –ü–æ–ª—É—á–∞–µ–º header_row –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                header_row = find_docs_header_row(file_data)
                logger.error(f"FIO column not found in docs-report file {filename} (UID {uid})")
                logger.error(f"  Header row: {header_row}")
                logger.error(f"  Columns: {list(df.columns)}")
                logger.error(f"  First 2 rows:\n{df.head(2).to_string()}")
                logger.warning(f"Skipping file {filename} (UID {uid}) - FIO column not found")
                continue
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ –§–ò–û
            df = df[df[fio_col].astype(str).str.strip() != ''].copy()
            
            if len(df) > 0:
                all_docs_dfs.append(df)
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á –≤ processed_keys –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                processed_keys[attachment_key] = time.time()
                logger.info(f"Found {len(df)} docs records in {filename} (UID {uid}, key: {attachment_key[:20]}...)")
            else:
                processed_keys[attachment_key] = time.time()
                logger.info(f"No docs records in {filename} (UID {uid}), but file processed")
        except Exception as e:
            logger.error(f"Error processing docs-report file {filename} (UID {uid}): {e}")
            import traceback
            traceback.print_exc()
    
    if not all_docs_dfs:
        logger.info("No docs-report records found")
        return
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö DataFrame
    df_all = pd.concat(all_docs_dfs, ignore_index=True)
    
    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
    fio_col = find_fio_column(df_all)
    if not fio_col:
        logger.error("FIO column not found in docs-report after merging all files")
        logger.error(f"  Columns: {list(df_all.columns)}")
        logger.error(f"  First 2 rows:\n{df_all.head(2).to_string()}")
        return
    
    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
    ttn_num_col = None
    for col in df_all.columns:
        col_norm = normalize_column_name(str(col))
        if '–Ω–æ–º–µ—Ä' in col_norm and '—Ç—Ç–Ω' in col_norm:
            ttn_num_col = col
            break
    
    ttn_date_col = None
    for col in df_all.columns:
        col_norm = normalize_column_name(str(col))
        if '–¥–∞—Ç–∞' in col_norm and '—Ç—Ç–Ω' in col_norm:
            ttn_date_col = col
            break
    
    route_num_col = None
    for col in df_all.columns:
        col_norm = normalize_column_name(str(col))
        if ('‚Ññ' in col_norm or '–Ω–æ–º–µ—Ä' in col_norm) and '–º–∞—Ä—à—Ä—É—Ç' in col_norm:
            route_num_col = col
            break
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á—É (–ù–æ–º–µ—Ä –¢–¢–ù, –î–∞—Ç–∞ –¢–¢–ù, –§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è, ‚Ññ –º–∞—Ä—à—Ä—É—Ç–∞)
    if ttn_num_col and ttn_date_col and fio_col:
        dedup_keys = set()
        df_unique_rows = []
        
        for _, row in df_all.iterrows():
            key = (
                str(row.get(ttn_num_col, '')),
                str(row.get(ttn_date_col, '')),
                str(row.get(fio_col, '')).strip(),
                str(row.get(route_num_col, '')) if route_num_col else ''
            )
            
            if key not in dedup_keys:
                dedup_keys.add(key)
                df_unique_rows.append(row)
        
        df_all = pd.DataFrame(df_unique_rows).reset_index(drop=True)
        logger.info(f"Total docs records before dedup: {sum(len(df) for df in all_docs_dfs)}, after dedup: {len(df_all)}")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –§–ò–û –≤–æ–¥–∏—Ç–µ–ª—è
    fio_series = df_all[fio_col].fillna("").astype(str).str.strip()
    df_all = df_all[fio_series != ''].copy()
    
    # –î–æ–±–∞–≤–ª—è–µ–º surname –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    df_all['_surname'] = df_all[fio_col].astype(str).str.split().str[0]
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ñ–∞–º–∏–ª–∏–∏ –∏ –§–ò–û
    df_all = df_all.sort_values(['_surname', fio_col]).reset_index(drop=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –≤–æ–¥–∏—Ç–µ–ª—é
    unique_fios = df_all[fio_col].unique()
    total_rows = len(df_all)
    logger.info(f"Preparing docs-report for {len(unique_fios)} drivers (total {total_rows} records)")
    
    # DRY_RUN —Ä–µ–∂–∏–º: –ª–æ–≥–∏—Ä—É–µ–º —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –±—ã–ª–æ –±—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ
    dry_run = config.get('dry_run', False)
    if dry_run:
        logger.info(f"[DRY_RUN] Would send {len(unique_fios)} messages to Telegram (topic docs=2)")
        for fio in unique_fios:
            df_driver = df_all[df_all[fio_col] == fio].copy()
            logger.info(f"[DRY_RUN] Would send message for driver {fio}: {len(df_driver)} records")
        return
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞—Ç—É –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π —Ç–∞–±–ª–∏—Ü
    topic_id = int(config.get('tg_topic_id_docs', 2))
    today_msk = datetime.now(ZoneInfo('Europe/Moscow')).date()
    date_str = today_msk.strftime('%d.%m.%Y')
    
    # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –¥–∞—Ç–æ–π
    if not dry_run:
        if send_telegram_message(config, date_str, topic_id=topic_id):
            logger.info(f"Sent date message: {date_str}")
        else:
            logger.error("Failed to send date message")
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π —Ç–∞–±–ª–∏—Ü
        time.sleep(0.3)
    
    sent_count = 0
    for fio in unique_fios:
        df_driver = df_all[df_all[fio_col] == fio].copy()
        df_driver = df_driver.drop(columns=['_surname'], errors='ignore')
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PNG
        temp_png = f'/tmp/docs_report_{hash(fio)}.png'
        if generate_png_table_docs(df_driver, temp_png):
            # Caption: "–û—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è [–§.–ò.–û. –≤–æ–¥–∏—Ç–µ–ª—è]"
            caption = f"–û—Ç—Å—Ç–∞—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è {fio}"
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram –≤ —Ç–µ–º—É 2
            topic_id = int(config.get('tg_topic_id_docs', 2))
            if send_telegram_photo(config, temp_png, caption, topic_id=topic_id):
                sent_count += 1
                logger.info(f"Sent docs-report for driver {fio}: {len(df_driver)} records")
            else:
                logger.error(f"Failed to send docs-report for driver {fio}")
            
            # Throttle –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ (0.3-0.6 —Å–µ–∫—É–Ω–¥—ã)
            time.sleep(0.5)
            
            # –û—á–∏—Å—Ç–∫–∞
            if os.path.exists(temp_png):
                os.remove(temp_png)
    
    logger.info(f"Docs-report completed: {sent_count}/{len(unique_fios)} messages sent")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏
    save_processed_keys(config['state_file'], processed_keys)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    config = load_config()
    
    logger.info("Starting late-report service")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π (–µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω FORCE_RESEND)
    force_resend = config.get('force_resend', False)
    if force_resend:
        logger.info("FORCE_RESEND: True -> state filtering disabled")
        processed_keys = {}
    else:
        processed_keys = load_processed_keys(config['state_file'])
        logger.info(f"Loaded {len(processed_keys)} processed keys from state")
    
    # DOCS_ONLY —Ä–µ–∂–∏–º
    docs_only = config.get('docs_only', False)
    if docs_only:
        logger.info("DOCS_ONLY enabled -> skipping LATE pipeline")
        config['run_late_report'] = False
        config['run_docs_report'] = True
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–∏–π –∏–∑ –ø–æ—á—Ç—ã
    attachments = get_email_attachments(config)
    
    if not attachments:
        logger.info("No new attachments found")
        return
    
    logger.info(f"Found {len(attachments)} Excel attachments")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π (–µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω FORCE_RESEND)
    new_attachments = []
    skipped_count = 0
    
    for uid, att_index, filename, file_data in attachments:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –≤–ª–æ–∂–µ–Ω–∏—è: uid:att_index:sha256
        file_hash = hashlib.sha256(file_data).hexdigest()
        attachment_key = f"{uid}:{att_index}:{file_hash}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω FORCE_RESEND)
        if not force_resend:
            if attachment_key in processed_keys:
                logger.debug(f"Skipping already processed attachment: {filename} (UID {uid}, key: {attachment_key[:20]}...)")
                skipped_count += 1
                continue
        
        new_attachments.append((uid, att_index, filename, file_data))
    
    logger.info(f"Filtered: {skipped_count} already processed, {len(new_attachments)} new attachments to process")
    
    if not new_attachments:
        logger.info("No new attachments to process")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º today_msk –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ DOCS –ø–æ —Ç–æ–∫–µ–Ω—É –¥–∞—Ç—ã
    # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω DOCS_DATE_TOKEN - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
    if config.get('docs_date_token'):
        date_token = config['docs_date_token']
        logger.info(f"DOCS date_token filter (override): {date_token}")
    else:
        report_tz = config.get('report_tz', 'Europe/Moscow')
        try:
            tz = ZoneInfo(report_tz)
        except Exception:
            tz = ZoneInfo('UTC')
        today_msk = datetime.now(tz).date()
        date_token = today_msk.strftime("%Y_%d_%m")  # —Ñ–æ—Ä–º–∞—Ç 2026_17_01
        logger.info(f"DOCS date_token filter: {date_token} (today_msk={today_msk})")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ late –∏ docs –æ—Ç—á—ë—Ç—ã
    late_attachments = []
    docs_attachments = []
    
    for uid, att_index, filename, file_data in new_attachments:
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –æ—Ç—á—ë—Ç–∞
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ late-report
            df_test = parse_excel(file_data)
            report_type = determine_report_type(df_test)
            
            # –î–ª—è DOCS: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω—É –¥–∞—Ç—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            if report_type == 'docs':
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ filename —Ç–æ–∫–µ–Ω –¥–∞—Ç—ã (–¥–∞–∂–µ –µ—Å–ª–∏ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã, —Ü–∏—Ñ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è)
                filename_str = str(filename) if filename else ''
                if date_token in filename_str:
                    docs_attachments.append((uid, att_index, filename, file_data))
                    logger.info(f"DOCS matched date_token={date_token}: UID {uid}, filename={filename[:50] if filename else 'N/A'}")
                else:
                    logger.debug(f"DOCS skipped (no date_token match): UID {uid}, filename={filename[:50] if filename else 'N/A'}, date_token={date_token}")
            elif report_type == 'late':
                late_attachments.append((uid, att_index, filename, file_data))
            else:
                logger.warning(f"Unknown report type for {filename} (UID {uid}), skipping")
        except Exception as e:
            logger.debug(f"Failed to determine report type for {filename} (UID {uid}): {e}, will try both parsers")
            # –ï—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ - –ø—Ä–æ–±—É–µ–º –æ–±–∞ —Ç–∏–ø–∞ (–Ω–æ –¥–ª—è docs –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω—É–∂–µ–Ω date_token)
            late_attachments.append((uid, att_index, filename, file_data))
            # –î–ª—è docs –ø—Ä–æ–≤–µ—Ä—è–µ–º date_token
            filename_str = str(filename) if filename else ''
            if date_token in filename_str:
                docs_attachments.append((uid, att_index, filename, file_data))
                logger.debug(f"DOCS matched date_token={date_token} (fallback): UID {uid}")
    
    logger.info(f"Classified: {len(late_attachments)} LATE, {len(docs_attachments)} DOCS attachments")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ late-report
    all_late_records = []
    late_processed_uids = []
    late_processed_hashes = []
    
    if config['run_late_report'] and late_attachments:
        logger.info(f"Processing {len(late_attachments)} late-report attachments")
        
        for uid, att_index, filename, file_data in late_attachments:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –≤–ª–æ–∂–µ–Ω–∏—è
            file_hash = hashlib.sha256(file_data).hexdigest()
            attachment_key = f"{uid}:{att_index}:{file_hash}"
            
            try:
                # –ü–∞—Ä—Å–∏–Ω–≥ Excel
                df = parse_excel(file_data)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ "–û–ø–æ–∑–¥–∞–Ω–∏–µ, –º–∏–Ω."
                if not has_valid_delay_column(df):
                    logger.warning(f"File {filename} (UID {uid}) does not contain '–û–ø–æ–∑–¥–∞–Ω–∏–µ, –º–∏–Ω.' column, skipping")
                    # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ, —á—Ç–æ–±—ã –Ω–µ –ø—ã—Ç–∞—Ç—å—Å—è —Å–Ω–æ–≤–∞
                    processed_keys[attachment_key] = time.time()
                    continue
            
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–æ–∑–¥–∞–≤—à–∏—Ö
                records = extract_late_records(df)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π: trim –¥–ª—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π, upper –¥–ª—è –≥–æ—Å–Ω–æ–º–µ—Ä–∞
                normalized_records = []
                for record in records:
                    normalized_record = {
                        'driver_name': str(record.get('driver_name', '')).strip(),
                        'plate_number': str(record.get('plate_number', '')).strip().upper(),
                        'route_name': str(record.get('route_name', '')).strip(),
                        'planned_time': str(record.get('planned_time', '')).strip(),
                        'assigned_time': str(record.get('assigned_time', '')).strip(),
                        'delay_minutes': int(record.get('delay_minutes', 0)),
                    }
                    normalized_records.append(normalized_record)
                
                if normalized_records:
                    all_late_records.extend(normalized_records)
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á –≤ processed_keys –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    processed_keys[attachment_key] = time.time()
                    logger.info(f"Found {len(normalized_records)} late records in {filename} (UID {uid}, key: {attachment_key[:20]}...)")
                else:
                    # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –æ–ø–æ–∑–¥–∞–≤—à–∏—Ö, –ø–æ–º–µ—á–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
                    processed_keys[attachment_key] = time.time()
                    logger.info(f"No late records in {filename} (UID {uid}), but file processed")
            except Exception as e:
                logger.error(f"Error processing {filename} (UID {uid}): {e}")
                import traceback
                traceback.print_exc()
    
    if not all_late_records and not config['send_if_empty']:
        logger.info("No late records found in all attachments")
        return
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ late-report —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –∏–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø—É—Å—Ç—ã—Ö –æ—Ç—á—ë—Ç–æ–≤
    if all_late_records or config['send_if_empty']:
        logger.info(f"Total late records before deduplication: {len(all_late_records)}")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ (fio, route_name, plan_time) –∏ –æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º delay
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        dedup_dict = {}
        for record in all_late_records:
            # –ö–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: (driver_name, route_name, planned_time)
            key = (
                record.get('driver_name', '').strip(),
                record.get('route_name', '').strip(),
                record.get('planned_time', '').strip()
            )
            
            # –ï—Å–ª–∏ —Ç–∞–∫–æ–π –∫–ª—é—á —É–∂–µ –µ—Å—Ç—å, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º delay –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å —Å –±–æ–ª—å—à–∏–º delay
            if key in dedup_dict:
                if record.get('delay_minutes', 0) > dedup_dict[key].get('delay_minutes', 0):
                    dedup_dict[key] = record
            else:
                dedup_dict[key] = record
        
        unique_records = list(dedup_dict.values())
        logger.info(f"Total late records after deduplication: {len(unique_records)}")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ delay –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        unique_records.sort(key=lambda x: x.get('delay_minutes', 0), reverse=True)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PNG
        temp_png = '/tmp/late_report.png'
        if generate_png_table(unique_records, temp_png):
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏
            caption = format_caption(unique_records)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram —Å topic_id –¥–ª—è late-report
            topic_id_late = int(config.get('tg_topic_id_late', 26))
            if send_telegram_photo(config, temp_png, caption, topic_id=topic_id_late):
                # –ï—Å–ª–∏ –ø–æ–¥–ø–∏—Å—å –æ–±—Ä–µ–∑–∞–ª–∞—Å—å, –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç–æ–º
                full_caption = format_caption(unique_records)
                if len(full_caption) > 1024:
                    remaining = '\n'.join([f"{get_delay_emoji(r['delay_minutes'])} {r['driver_name']} ‚Äî {r['delay_minutes']}" 
                                          for r in unique_records[len(caption.split('\n')):]])
                    send_telegram_text(config, remaining)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏
                save_processed_keys(config['state_file'], processed_keys)
                
                logger.info(f"Successfully processed late-report with {len(unique_records)} unique late records")
            else:
                logger.error("Failed to send late-report to Telegram")
        
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if os.path.exists(temp_png):
            os.remove(temp_png)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ docs-report
    if config['run_docs_report'] and docs_attachments:
        logger.info(f"Processing {len(docs_attachments)} docs-report attachments")
        process_docs_report(config, docs_attachments, processed_keys)


if __name__ == '__main__':
    main()
